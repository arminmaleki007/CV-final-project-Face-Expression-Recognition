# Facial Expression-Recognition
## Part 1: Conceptual design
The objective of this project is online facial expression recognition using the camera. We can use any camera-based system like a laptop webcam or a simple Raspberry Pi system to recognize facial expressions. The importance of this work relies on the fact that emotions change the facial expression of the face. Using this system can help us to know people's reactions in different situations.

To do this task, we need to design a computer vision method using deep learning to extract features from faces that are different for different facial expressions. For example. the shape of lips, eyes, brows, and face lines are different for different facial expressions. The color of features mustn't affect the classification tasks, and it is better to use gray-scale pictures to speed up the training process. Detecting these changes is necessary to do the task. At last, we need to use these features to **classify** the facial expression of the person in the camera view. 

I am thinking about two solutions for doing this task. The first solution is using deep CNN networks for feature extraction and MLP networks for classification. In addition, we need to define a class for each expression in the dataset plus a not detected class for expressions with low similarities to the goal classes. We need to tune the system's hyperparameters (like the number of layers, the number of nodes in each layer, batch size, activation function, and even the optimizer) for the best performance. The second solution is to use an encoder-decoder algorithm using transformers. This architecture uses a latent space for each class of facial expressions. Each point in the latent space corresponds to a class. This is a proposal for the solution method and requires a deeper understanding of the methods and implementation. 

I found a dataset with 8 different classes for facial expressions. The dataset contains Angry, Contempt, Disgust, Fear, Happy, Neutral, Sadness, and Surprise. There are multiple pictures from video frames of faces from different people with different facial expressions. We need to divide the dataset by people, not pictures into training (70%), validation (15%), and test sets (15%). This is because using the same person in training, validation, and testing, will result in overfitting without knowing. That being said, I believe we can find other datasets with more classes and more pictures by spending more time. On the other hand, we can use video frames as input for RNN networks after feature extraction using CNN. This will help the classification problem by using data series with RNN networks. In addition, I plan to use data augmentation. This increases data in the dataset, which will improve the performance of the system by allowing us to use deeper networks without overfitting.

For this problem, we need to use cross-entropy loss. In addition, we need to use accuracy and loss as a metric for the performance evaluation. We can use the confusion matrix as a good metric too. We can try different activation functions for network layers. However, I think ReLU or Leaky ReLU would work fine for all layers except the last layer which we need to use the sigmoid function. In addition, ADAM optimizer may work best for this task. 

My plan is to use my laptop for training. My laptop has a 6 GB Nvidia RTX 3060 GPU which I think is enough for training for this task. The alternative environment is CRC. In addition, I think Google Colab may work too. However, it will be slower than other environments.
